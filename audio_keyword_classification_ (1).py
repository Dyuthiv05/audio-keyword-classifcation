# -*- coding: utf-8 -*-
"""Audio keyword classification .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vtl66lJa3mSf0-ECtYJERq05TSdhLMtj
"""

!pip install datasets
!pip install snntorch
!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

from datasets import load_dataset
import torchaudio
import torch
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence
import snntorch as snn
from snntorch import spikegen
import torch.nn as nn
from torch.optim import Adam

dataset = load_dataset("speech_commands", "v0.01", keep_in_memory=True)

print(type(dataset))
print(dataset)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

import torchaudio
import torch


def preprocess(batch):
    waveform = torch.tensor(batch['audio']['array'], dtype=torch.float32).unsqueeze(0)  # Ensure it has a batch dimension
    sample_rate = batch['audio']['sampling_rate']
    mfcc_transform = torchaudio.transforms.MFCC(
        sample_rate=sample_rate,
        n_mfcc=13,
        melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': 23, 'normalized': True}
    )
    mfcc = mfcc_transform(waveform)
    return {"mfcc": mfcc.squeeze(0)}

dataset = dataset.map(preprocess, remove_columns=['file', 'audio', 'is_unknown', 'speaker_id', 'utterance_id'])

from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence

def collate_fn(batch):
    # Collect all MFCCs and labels from the batch
    mfccs = [torch.tensor(item['mfcc'], dtype=torch.float).T for item in batch]
    labels = [torch.tensor(item['label'], dtype=torch.long) for item in batch]

    # Pad the MFCC sequences to have the same length
    mfccs_padded = pad_sequence(mfccs, batch_first=True)

    # Stack labels into a single tensor
    labels_tensor = torch.stack(labels)

    #print(mfccs_padded.shape)
    mfccs_padded = mfccs_padded.permute(0, 2, 1)

    #for mfcc in mfccs_padded:
    #  print(mfcc.shape)

    return mfccs_padded, labels_tensor

#print(dataset['train']['mfcc'][0])

dataloader = DataLoader(dataset['train'], batch_size=32, collate_fn=collate_fn, shuffle=True)

class AudioCNN(nn.Module):
    def __init__(self):
        super(AudioCNN, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=13, out_channels=32, kernel_size=5, padding=2)
        self.pool = nn.MaxPool1d(2)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, padding=2)
        self.fc1 = None  # Initialize later in the forward pass
        self.fc2 = nn.Linear(128, 31)

    def forward(self, x):
        x = self.conv1(x)
        #print("After conv1:", x.shape)
        x = self.pool(x)
        #print("After pool1:", x.shape)
        x = self.relu(x)
        x = self.conv2(x)
        #print("After conv2:", x.shape)
        x = self.pool(x)
        #print("After pool2:", x.shape)
        x = self.relu(x)
        x = x.view(x.size(0), -1)  # Flatten the tensor for the fully connected layer
        #print("Before fc1:", x.shape)

        # Dynamically create fc1 with the correct input size if not initialized
        if not self.fc1:
            self.fc1 = nn.Linear(x.shape[1], 128).to(x.device)
            #print("Initialized fc1:", self.fc1)

        x = self.fc1(x)
        x = self.fc2(x)
        return x

# Initialize the model and transfer it to the device
model = AudioCNN().to(device)
optimizer = Adam(model.parameters(), lr=0.001)
loss_fn = nn.CrossEntropyLoss()

num_epochs = 10
for epoch in range(num_epochs):
    for mfccs, labels in dataloader:
        #print(mfccs.shape)
        if mfccs.shape == torch.Size([32, 13, 101]):
          optimizer.zero_grad()
          output = model(mfccs)  # Forward pass
          loss = loss_fn(output, labels)
          loss.backward()
          optimizer.step()
        else:
          print('invalid size')

    print(f"Epoch {epoch + 1}, Loss {loss.item()}")